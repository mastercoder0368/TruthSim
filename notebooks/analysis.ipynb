{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruthSim Results Analysis\n",
    "\n",
    "This notebook analyzes the results from TruthSim diagnostic conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation results\n",
    "eval_path = Path('../data/evaluations/evaluation_detailed.json')\n",
    "\n",
    "if eval_path.exists():\n",
    "    with open(eval_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    print(f\"Loaded results for {results['transcripts_evaluated']} transcripts\")\n",
    "else:\n",
    "    print(\"No evaluation results found. Run evaluate.py first.\")\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Diagnostic Accuracy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results and 'diagnosis_matching' in results:\n",
    "    # Create DataFrame\n",
    "    df_diagnosis = pd.DataFrame(results['diagnosis_matching'])\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = df_diagnosis['match'].mean()\n",
    "    print(f\"Overall Diagnostic Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    # Display sample results\n",
    "    display(df_diagnosis[['patient_id', 'doctor_diagnosis', 'ground_truth', 'match', 'reasoning']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulation Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results and 'llm_judge' in results:\n",
    "    # Create DataFrame\n",
    "    df_judge = pd.DataFrame(results['llm_judge'])\n",
    "    \n",
    "    # Extract scores\n",
    "    df_judge['truth_pass'] = df_judge.apply(\n",
    "        lambda x: x.get('truth_preservation', {}).get('pass', False), axis=1\n",
    "    )\n",
    "    df_judge['avg_realism'] = df_judge.apply(\n",
    "        lambda x: x.get('realism', {}).get('average', 0), axis=1\n",
    "    )\n",
    "    df_judge['avg_utility'] = df_judge.apply(\n",
    "        lambda x: x.get('clinical_utility', {}).get('average', 0), axis=1\n",
    "    )\n",
    "    \n",
    "    print(f\"Truth Preservation Pass Rate: {df_judge['truth_pass'].mean():.2%}\")\n",
    "    print(f\"Average Realism Score: {df_judge['avg_realism'].mean():.2f}/5.0\")\n",
    "    print(f\"Average Clinical Utility: {df_judge['avg_utility'].mean():.2f}/5.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results and 'llm_judge' in results:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Realism distribution\n",
    "    axes[0].hist(df_judge['avg_realism'].dropna(), bins=10, edgecolor='black')\n",
    "    axes[0].set_xlabel('Realism Score')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Distribution of Realism Scores')\n",
    "    axes[0].axvline(df_judge['avg_realism'].mean(), color='red', linestyle='--', label='Mean')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Utility distribution\n",
    "    axes[1].hist(df_judge['avg_utility'].dropna(), bins=10, edgecolor='black')\n",
    "    axes[1].set_xlabel('Utility Score')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('Distribution of Clinical Utility Scores')\n",
    "    axes[1].axvline(df_judge['avg_utility'].mean(), color='red', linestyle='--', label='Mean')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Truth preservation\n",
    "    truth_counts = df_judge['truth_pass'].value_counts()\n",
    "    axes[2].pie(truth_counts, labels=['Pass', 'Fail'], autopct='%1.1f%%', colors=['green', 'red'])\n",
    "    axes[2].set_title('Truth Preservation')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/evaluations/quality_distribution.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison (if multiple models evaluated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transcripts to get model information\n",
    "conversations_dir = Path('../data/conversations')\n",
    "\n",
    "if conversations_dir.exists():\n",
    "    model_results = {}\n",
    "    \n",
    "    for model_dir in conversations_dir.iterdir():\n",
    "        if model_dir.is_dir():\n",
    "            model_name = model_dir.name\n",
    "            transcripts = list(model_dir.glob('*.json'))\n",
    "            \n",
    "            if transcripts:\n",
    "                model_results[model_name] = {\n",
    "                    'count': len(transcripts),\n",
    "                }\n",
    "    \n",
    "    if model_results:\n",
    "        print(\"Models evaluated:\")\n",
    "        for model, info in model_results.items():\n",
    "            print(f\"  - {model}: {info['count']} conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    print(\"=\" * 50)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    metrics = results.get('metrics', {})\n",
    "    \n",
    "    if 'diagnostic_accuracy' in metrics:\n",
    "        acc = metrics['diagnostic_accuracy']\n",
    "        print(f\"\\nDiagnostic Accuracy: {acc['top1_accuracy']:.2%} ({acc['correct']}/{acc['total']})\")\n",
    "    \n",
    "    if 'simulation_quality' in metrics:\n",
    "        sq = metrics['simulation_quality']\n",
    "        print(f\"\\nSimulation Quality:\")\n",
    "        print(f\"  Truth Preservation: {sq['truth_preservation']['pass_rate']:.2%}\")\n",
    "        print(f\"  Hallucination Rate: {sq['truth_preservation']['hallucination_rate']:.2%}\")\n",
    "        print(f\"  Average Realism: {sq['realism']['mean']:.2f} ± {sq['realism']['std']:.2f}\")\n",
    "        print(f\"  Average Utility: {sq['clinical_utility']['mean']:.2f} ± {sq['clinical_utility']['std']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
